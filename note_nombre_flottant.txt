note https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html.

The most common situation is illustrated by the decimal number 0.1. 
'Although it has a finite decimal representation, in binary it has an 
infinite repeating representation.
 
https://stackoverflow.com/questions/3448777/how-to-represent-0-1-in-floating-point-arithmetic-and-decimal
https://www.h-schmidt.net/FloatConverter/IEEE754.html
https://en.wikipedia.org/wiki/IEEE_754-1985
https://en.wikipedia.org/wiki/Guard_digit
http://0.30000000000000004.com/

Your language isn't broken, it's doing floating point math. Computers can only 
natively store integers, so they need some way of representing decimal numbers. 
This representation comes with some degree of inaccuracy. That's why, more often 
than not, .1 + .2 != .3.

Why does this happen? It's actually pretty simple. When you have a base 10 
system (like ours), it can only express fractions that use a prime factor of the 
base. The prime factors of 10 are 2 and 5. So 1/2, 1/4, 1/5, 1/8, and 1/10 can 
all be expressed cleanly because the denominators all use prime factors of 10. 
In contrast, 1/3, 1/6, and 1/7 are all repeating decimals because their 
denominators use a prime factor of 3 or 7. In binary (or base 2), the only prime 
factor is 2. So you can only express fractions cleanly which only contain 2 as a 
prime factor. In binary, 1/2, 1/4, 1/8 would all be expressed cleanly as 
decimals. While, 1/5 or 1/10 would be repeating decimals. So 0.1 and 0.2 (1/10 
and 1/5) while clean decimals in a base 10 system, are repeating decimals in the 
base 2 system the computer is operating in. When you do math on these repeating 
decimals, you end up with leftovers which carry over when you convert the 
computer's base 2 (binary) number into a more human readable base 10 number.


If the leading digit is nonzero, then the representation is said to be 
normalized. Requiring that a floating-point representation be normalized makes 
the representation unique. Since floating-point numbers are always normalized, 
the most significant bit of the significand is always 1, and there is no reason 
to waste a bit of storage representing it. Formats that use this trick are said 
to have a hidden bit.

IEEE 754 single precision is encoded in 32 bits using 
- 1 bit for the sign, 
- 8 bits for the exponent, and 
- 23 bits for the significand. 
However, it uses a hidden bit, so the significand is 24 bits (p = 24), even 
though it is encoded using only 23 bits. 

s eeeeeeee mmmmmmmmmmmmmmmmmmmmmmm    1/n
0 01111011 10011001100110011001101	=> chiffre en 32 bits.

		   
s: signe du chiffre (positif ou négatif)
Sur 1 bit
0: positif
1: négatif

0
1	1
10	2
11	3
100 4
101 5
110 6
111 7
1000 8


eeeeeeee: Exposant ou multiplier (la puissance à laquelle est poussé le chiffre)
ou biased exponent - trouvé par la formule 2^(k-1) - 1 => 2^(8 -1) - 1 = 127
sur 8 bits 0 -> 255
  00000000	=> 0
+ 01000000	=> 64
+ 00100000  => 32
+ 00010000	=> 16
+ 00001000  => 8
+ 00000010  => 2
+ 00000001	=> 1
= 01111011  => 123
123 -127 bias = -4 => multiplier = 2^-4 ou 1/16.

mmmmmmmmmmmmmmmmmmmmmmm: mantisse ou fraction
sur 23 bits: 1/2 -> 1/8388608
It consists of 1 (the implicit base) plus (for all those bits with each being 
worth 1/(2^n) as n starts at 1 and increases to the right). Remember that 1 bit
is hidden (see normalized text above)
  10000000000000000000000 => 1/2 (1/2^1) 
+ 00010000000000000000000 => 1/16 (1/2^4)
+ 00001000000000000000000 => 1/32 (1/2^5)	
+ 00000001000000000000000 => 1/256
+ 00000000100000000000000 => 1/512
+ 00000000000100000000000 => 1/4096
+ 00000000000010000000000 => 1/8192
+ 00000000000000010000000 => 1/65536
+ 00000000000000001000000 => 1/131072 
+ 00000000000000000001000 => 1/1048576
+ 00000000000000000000100 => 1/2097152
+ 00000000000000000000001 => 1/8388608 (1/2^23) 
= 10011001100110011001101 => 1.60000002384185791015625

mantisse * exposant 
 = 1.60000002384185791015625 * 1/16 = 0.100000001490116119384765625